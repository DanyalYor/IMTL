{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1553cb2f",
   "metadata": {},
   "source": [
    "# Settings script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "MODEL_SCRATCH          = False      # No pretrained weights, trained from scratch on Food-101\n",
    "MODEL_BASELINE         = False      # Pretrained EfficientNet, but NOT trained on Food-101\n",
    "MODEL_FINETUNE_PARTIAL = False      # Pretrained EfficientNet, THEN fine-tuned on Food-101\n",
    "MODEL_FINETUNE         = True       # Pretrained EfficientNet, THEN fine-tuned on Food-101\n",
    "\n",
    "# Augmentation settings\n",
    "AUGMENTATION_MIN      = False\n",
    "AUGMENTATION_MANUAL   = False\n",
    "AUGMENTATION_AUTO     = False\n",
    "AUGMENTATION_IMAGENET = True\n",
    "\n",
    "# Training settings (Pick one)\n",
    "REGULAR_TRAINING = True\n",
    "RAYTUNE_TRAINING = False            \n",
    "LOGGING_ACTIVE = REGULAR_TRAINING   # Only perform logging with regular training\n",
    "\n",
    "# Other settings\n",
    "USE_FULL_DATASET = False            # If false, use small dataset instead (10K pictures, 100 per class)\n",
    "NUM_EPOCHS       = 15\n",
    "BATCH_SIZE       = 32\n",
    "LEARN_RATE       = 1e-3\n",
    "\n",
    "# Sanity checks\n",
    "assert sum([MODEL_BASELINE, MODEL_SCRATCH, MODEL_FINETUNE_PARTIAL ,MODEL_FINETUNE]) == 1, \\\n",
    "    \"Exactly one model must be selected.\"\n",
    "\n",
    "assert sum([REGULAR_TRAINING, RAYTUNE_TRAINING]) == 1, \\\n",
    "    \"Exactly one training mode must be active.\"\n",
    "\n",
    "assert not (RAYTUNE_TRAINING and LOGGING_ACTIVE), \\\n",
    "    \"LOGGING_ACTIVE should be False during Ray Tune.\"\n",
    "\n",
    "assert not (MODEL_BASELINE and (REGULAR_TRAINING or RAYTUNE_TRAINING)), \\\n",
    "    \"MODEL_BASELINE can't be used with TRAINING.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6492d",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "Run this in the terminal.\n",
    "Remember to check if the intended kernel is in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e73c78",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56810e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from collections import Counter\n",
    "import copy\n",
    "import tempfile\n",
    "from math import isnan\n",
    "\n",
    "# ML\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TorchVision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "\n",
    "# W&B\n",
    "import wandb\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ray Tune\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11ed7",
   "metadata": {},
   "source": [
    "## Set GPU variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "device = \"cuda\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7374b0a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f85172",
   "metadata": {},
   "source": [
    "## Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223add30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = pathlib.Path().resolve()\n",
    "\n",
    "if USE_FULL_DATASET:\n",
    "    IMAGE_DIR = ROOT / \"data\" / \"food-101-big\" / \"images\"\n",
    "else:\n",
    "    IMAGE_DIR = ROOT / \"data\" / \"food-101-small\"\n",
    "\n",
    "print(\"Using IMAGE_DIR =\", IMAGE_DIR)\n",
    "\n",
    "if not IMAGE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing dataset folder:\\n{IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338636e",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224 # Needed width and height dimensions for EfficientNet\n",
    "\n",
    "imagenet_norm = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "\n",
    "# Augment data\n",
    "if AUGMENTATION_MIN:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "elif AUGMENTATION_MANUAL:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()\n",
    "    ])    \n",
    "elif AUGMENTATION_AUTO:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        AutoAugment(policy=AutoAugmentPolicy.IMAGENET),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "elif AUGMENTATION_IMAGENET:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        imagenet_norm,\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError(\"No augmentation mode selected.\")\n",
    "\n",
    "if AUGMENTATION_IMAGENET:\n",
    "    validation_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        imagenet_norm,\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        imagenet_norm,\n",
    "    ])\n",
    "\n",
    "else:\n",
    "    validation_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(IMAGE_DIR, transform=None)\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"Total images:\", len(full_dataset))\n",
    "print(\"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc7548",
   "metadata": {},
   "source": [
    "## Split data into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(full_dataset.targets)   # labels\n",
    "indices = np.arange(len(targets))\n",
    "\n",
    "# split: 70% train, 30% temp\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.30,\n",
    "    stratify=targets,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# split temp into 15% val, 15% test\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.50,                       # 0.50 * 30% = 15%\n",
    "    stratify=targets[temp_idx],\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a93aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = copy.deepcopy(full_dataset)\n",
    "val_dataset   = copy.deepcopy(full_dataset)\n",
    "test_dataset  = copy.deepcopy(full_dataset)\n",
    "\n",
    "train_dataset.transform = train_transform\n",
    "val_dataset.transform   = validation_transform\n",
    "test_dataset.transform  = test_transform\n",
    "\n",
    "# Wrap in Subsets\n",
    "train_ds = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "val_ds   = torch.utils.data.Subset(val_dataset, val_idx)\n",
    "test_ds  = torch.utils.data.Subset(test_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_split_balance(subset, name):\n",
    "    labels = [full_dataset.targets[i] for i in subset.indices]\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    unique_counts = set(counts.values())\n",
    "    if len(unique_counts) == 1:\n",
    "        # perfectly balanced\n",
    "        value = unique_counts.pop()\n",
    "        print(f\"{name} distribution: {value} per class\")\n",
    "    else:\n",
    "        # imbalance detected\n",
    "        print(f\"{name} distribution is NOT balanced:\")\n",
    "        print(counts)\n",
    "\n",
    "check_split_balance(train_ds, \"Train\")\n",
    "check_split_balance(val_ds, \"Val\")\n",
    "check_split_balance(test_ds, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a56d32",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGULAR_TRAINING:\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    print(\"Train samples:\", len(train_ds))\n",
    "    print(\"Val samples:  \", len(val_ds))\n",
    "    print(\"Test samples: \", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdda7b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db9857",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model but NOT trained (baseline)\n",
    "def build_model_baseline(lr, num_classes):\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    # Freeze all parameters → no training\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    optimizer = None \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "# Train from scratch\n",
    "def build_model_scratch(lr, num_classes):\n",
    "    model = efficientnet_b0(weights=None)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "def build_model_finetune_partial(lr, num_classes):\n",
    "    \"\"\"\n",
    "    Pretrained EfficientNet-B0 with partial fine-tuning:\n",
    "    - Freeze early feature extractor blocks\n",
    "    - Train only later blocks + classifier\n",
    "    \"\"\"\n",
    "\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Replace classifier head\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    # --- Freeze early layers (blocks 0–4) ---\n",
    "    for idx, block in enumerate(model.features):\n",
    "        if idx <= 4:\n",
    "            for p in block.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            # idx >= 5 → unfreeze\n",
    "            for p in block.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    # classifier stays trainable\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "# Pretrained → fine-tuned\n",
    "def build_model_finetune(lr, num_classes):\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "\n",
    "# def build_model_1(lr, num_classes):\n",
    "#     \"\"\"EfficientNet-B0 model\"\"\"\n",
    "#     weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "#     model = efficientnet_b0(weights=weights)\n",
    "\n",
    "#     model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "#     for p in model.features.parameters():\n",
    "#         p.requires_grad = False\n",
    "\n",
    "#     model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     return model, optimizer, criterion\n",
    "\n",
    "\n",
    "# def build_model_2(lr, num_classes):\n",
    "#     \"\"\"Placeholder model — intentionally blank\"\"\"\n",
    "#     raise NotImplementedError(\"MODEL_2 not implemented yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc65284",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_BASELINE:\n",
    "    build_model = build_model_baseline\n",
    "elif MODEL_SCRATCH:\n",
    "    build_model = build_model_scratch\n",
    "elif MODEL_FINETUNE_PARTIAL:\n",
    "    build_model = build_model_finetune_partial\n",
    "elif MODEL_FINETUNE:\n",
    "    build_model = build_model_finetune\n",
    "\n",
    "if REGULAR_TRAINING:\n",
    "    model, optimizer, criterion = build_model(LEARN_RATE, num_classes)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb85285",
   "metadata": {},
   "source": [
    "# Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGGING_ACTIVE:\n",
    "    wandb.login()\n",
    "\n",
    "    batch_size = BATCH_SIZE\n",
    "    learning_rate = LEARN_RATE\n",
    "\n",
    "    if MODEL_SCRATCH:\n",
    "        model_name = \"scratch\"\n",
    "        PRETRAINED = False\n",
    "        FINETUNED = False\n",
    "    elif MODEL_BASELINE:\n",
    "        model_name = \"baseline\"\n",
    "        PRETRAINED = True\n",
    "        FINETUNED = False\n",
    "    elif MODEL_FINETUNE_PARTIAL:\n",
    "        model_name = \"finetune_partial\"\n",
    "        PRETRAINED = True\n",
    "        FINETUNED = True\n",
    "    elif MODEL_FINETUNE:\n",
    "        model_name = \"finetune\"\n",
    "        PRETRAINED = True\n",
    "        FINETUNED = True\n",
    "    else:\n",
    "        model_name = \"\"\n",
    "\n",
    "    if AUGMENTATION_MIN:\n",
    "        augmentation_string = \"noaugment_\"\n",
    "    elif AUGMENTATION_MANUAL:\n",
    "        augmentation_string = \"manualaugment_\"\n",
    "    elif AUGMENTATION_AUTO:\n",
    "        augmentation_string = \"autoaugment_\"\n",
    "    else:\n",
    "        augmentation_string = \"\"\n",
    "\n",
    "    def log_metric(data: dict, step=None):\n",
    "        wandb.log(data, step=step)\n",
    "\n",
    "    def log_batch_metric(loss_value, optimizer, epoch):\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"batch/train_loss\": loss_value,\n",
    "                \"batch/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "            },\n",
    "            step=epoch\n",
    "        )\n",
    "\n",
    "    def log_accuracy_vs_epoch(split: str, acc_value: float, epoch: int):\n",
    "        wandb.log({f\"accuracy_vs_epoch/{split}\": acc_value}, step=epoch)\n",
    "\n",
    "\n",
    "    run_name = (\n",
    "        f\"model{model_name}_\"\n",
    "        f\"lr{learning_rate}_bs{batch_size}_\"\n",
    "        f\"{'finetuned_' if FINETUNED else ''}\"\n",
    "        f\"{'pretrained_' if PRETRAINED else ''}\"\n",
    "        f\"{augmentation_string}\"\n",
    "    )\n",
    "\n",
    "    wandb_config = {\n",
    "        \"model_name\": \"efficientnet_b0\",\n",
    "        \"img_size\": img_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"train_size\": len(train_ds),\n",
    "        \"val_size\": len(val_ds),\n",
    "        \"test_size\": len(test_ds),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"transfer_learning\": True,\n",
    "        \"feature_extractor_frozen\": True,\n",
    "        \"augmentation\": \"AutoAugment_IMAGENET\",\n",
    "    }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"IMTL\",\n",
    "        config=wandb_config,\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0ed0d",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ff7f3",
   "metadata": {},
   "source": [
    "## Define Regular training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGULAR_TRAINING:\n",
    "    \n",
    "    def train_one_epoch(model, loader, optimizer, criterion, device, epoch, log_interval=100):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "\n",
    "        for batch_idx, (imgs, labels) in enumerate(pbar):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            avg_loss_so_far = total_loss / total\n",
    "            avg_acc_so_far = correct / total\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_loss_so_far:.4f}\",\n",
    "                \"acc\": f\"{100 * avg_acc_so_far:.2f}%\"\n",
    "            })\n",
    "\n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                log_batch_metric(loss.item(), optimizer, epoch)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "\n",
    "        log_metric({\"loss/train\": avg_loss, \"accuracy/train\": avg_acc}, step=epoch)\n",
    "        log_accuracy_vs_epoch(\"train\", avg_acc, epoch)\n",
    "\n",
    "        return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "    def evaluate(model, loader, criterion, device, epoch=None, split=\"val\"):\n",
    "        model.eval()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        desc = f\"Evaluating ({split})\"\n",
    "        if epoch is not None:\n",
    "            desc += f\" Epoch {epoch}\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(loader, desc=desc, leave=False)\n",
    "\n",
    "            for imgs, labels in pbar:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item() * imgs.size(0)\n",
    "                _, preds = outputs.max(1)\n",
    "                correct += preds.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": f\"{total_loss / total:.4f}\",\n",
    "                    \"acc\": f\"{100 * (correct / total):.2f}%\"\n",
    "                })\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "\n",
    "        log_metric(\n",
    "            {\n",
    "                f\"{split}/loss\": avg_loss,\n",
    "                f\"{split}/accuracy\": avg_acc,\n",
    "                f\"{split}/error_rate\": 1 - avg_acc,\n",
    "                **({\"epoch\": epoch} if epoch is not None else {})\n",
    "            },\n",
    "            step=epoch\n",
    "        )\n",
    "\n",
    "        if split == \"val\" and epoch is not None:\n",
    "            log_accuracy_vs_epoch(\"val\", avg_acc, epoch)\n",
    "\n",
    "        return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e6387",
   "metadata": {},
   "source": [
    "## Define RayTune training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAYTUNE_TRAINING:\n",
    "    \n",
    "    def train_one_epoch_raytune(model, loader, optimizer, criterion, device):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    def evaluate_raytune(model, loader, criterion, device):\n",
    "        model.eval()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(loader, desc=\"Evaluating (raytune)\", leave=False)\n",
    "\n",
    "            for imgs, labels in pbar:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item() * imgs.size(0)\n",
    "                _, preds = outputs.max(1)\n",
    "                correct += preds.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": f\"{total_loss / total:.4f}\",\n",
    "                    \"acc\": f\"{100 * (correct / total):.2f}%\"\n",
    "                })\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "\n",
    "        return avg_loss, avg_acc\n",
    "\n",
    "    def tune_train(config):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        lr = config[\"lr\"]\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        model, optimizer, criterion = build_model(lr, num_classes)\n",
    "\n",
    "        # Move to GPU\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            # Run epoch and time it\n",
    "            start_time = time.time()\n",
    "            train_one_epoch_raytune(model, train_loader, optimizer, criterion, device)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            # Evaluation\n",
    "            val_loss, val_acc = evaluate_raytune(model, val_loader, criterion, device)\n",
    "\n",
    "            # GPU memory logging\n",
    "            gpu_mem = 0.0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            # Report metrics\n",
    "            tune.report({\n",
    "                \"loss\": float(val_loss),\n",
    "                \"accuracy\": float(val_acc),\n",
    "                \"epoch_time\": float(epoch_time),\n",
    "                \"gpu_mem\": float(gpu_mem),\n",
    "                \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dff62",
   "metadata": {},
   "source": [
    "## Regular training (If toggled on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGULAR_TRAINING:\n",
    "    if optimizer is None:\n",
    "        print(\"=== Baseline model selected — skipping training ===\")\n",
    "\n",
    "    else:\n",
    "        # Save accuracies for each run\n",
    "        train_acc_history = []\n",
    "        val_acc_history   = []\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "            val_loss, val_acc     = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "            print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61756a68",
   "metadata": {},
   "source": [
    "## Raytune training (If toggled on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e984a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAYTUNE_TRAINING:\n",
    "    os.environ[\"RAY_DISABLE_METRICS_EXPORT\"] = \"1\"\n",
    "\n",
    "    search_space = {\n",
    "        #\"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "        \"lr\": tune.loguniform(3e-5, 3e-3),\n",
    "        \"batch_size\": tune.choice([16, 32, 64]),\n",
    "        \"epochs\":  tune.randint(3, 15),\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=15,\n",
    "        grace_period=2,\n",
    "        reduction_factor=3,\n",
    "    )\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"epoch_time\", \"gpu_mem\"]\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune_train,\n",
    "            resources={\"cpu\": 4, \"gpu\": 1 if torch.cuda.is_available() else 0},\n",
    "        ),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            num_samples=6,\n",
    "        ),\n",
    "        run_config=tune.RunConfig(progress_reporter=reporter),\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n",
    "\n",
    "    print(\"Best config:\", best.config)\n",
    "    best_config = best.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc8cfb",
   "metadata": {},
   "source": [
    "### Write RayTune result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de338c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAYTUNE_TRAINING:\n",
    "\n",
    "    summary_path = ROOT / \"raytune\" / \"raytune_summary.txt\"\n",
    "\n",
    "    best_result = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n",
    "    analysis = results._experiment_analysis\n",
    "    trials = analysis.trials  # raw Trial objects\n",
    "\n",
    "    # Helper to format floats like Ray does\n",
    "    def fmt(x, width):\n",
    "        if isinstance(x, float):\n",
    "            if isnan(x):\n",
    "                return \"nan\".ljust(width)\n",
    "            # scientific notation for small lr values, otherwise 4 decimals\n",
    "            if abs(x) < 1e-3:\n",
    "                return f\"{x:.2e}\".ljust(width)\n",
    "            return f\"{x:.4f}\".ljust(width)\n",
    "        return str(x).ljust(width)\n",
    "\n",
    "\n",
    "    # Column widths\n",
    "    W_NAME = 24\n",
    "    W_STATUS = 12\n",
    "    W_BATCH = 12\n",
    "    W_LR = 12\n",
    "    W_LOSS = 10\n",
    "    W_ACC = 12\n",
    "    W_TIME = 12\n",
    "    W_MEM = 10\n",
    "\n",
    "    # Prepare table header\n",
    "    header = (\n",
    "        f\"| {'Trial name'.ljust(W_NAME)}\"\n",
    "        f\"| {'status'.ljust(W_STATUS)}\"\n",
    "        f\"| {'batch_size'.ljust(W_BATCH)}\"\n",
    "        f\"| {'lr'.ljust(W_LR)}\"\n",
    "        f\"| {'loss'.ljust(W_LOSS)}\"\n",
    "        f\"| {'accuracy'.ljust(W_ACC)}\"\n",
    "        f\"| {'epoch_time'.ljust(W_TIME)}\"\n",
    "        f\"| {'gpu_mem'.ljust(W_MEM)} |\\n\"\n",
    "    )\n",
    "\n",
    "    separator = \"+\" + \"-\"*(len(header)-3) + \"+\\n\"\n",
    "\n",
    "\n",
    "    # Build the ASCII table\n",
    "    table = separator\n",
    "    table += header\n",
    "    table += separator\n",
    "\n",
    "    for trial in trials:\n",
    "        r = trial.last_result\n",
    "\n",
    "        row = (\n",
    "            f\"| {trial.trial_id.ljust(W_NAME)}\"\n",
    "            f\"| {trial.status.ljust(W_STATUS)}\"\n",
    "            f\"| {str(trial.config.get('batch_size', '-')).ljust(W_BATCH)}\"\n",
    "            f\"| {fmt(trial.config.get('lr', '-'), W_LR)}\"\n",
    "            f\"| {fmt(r.get('loss', '-'), W_LOSS)}\"\n",
    "            f\"| {fmt(r.get('accuracy', '-'), W_ACC)}\"\n",
    "            f\"| {fmt(r.get('epoch_time', '-'), W_TIME)}\"\n",
    "            f\"| {fmt(r.get('gpu_mem', '-'), W_MEM)} |\\n\"\n",
    "        )\n",
    "        table += row\n",
    "\n",
    "    table += separator\n",
    "\n",
    "    # Save everything to the file\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Ray Tune Summary (ASCII Table) ===\\n\\n\")\n",
    "        f.write(f\"Result logdir: {best_result.path}\\n\")\n",
    "        f.write(f\"Number of trials: {len(trials)}\\n\\n\")\n",
    "        f.write(table)\n",
    "        f.write(\"\\nBest config:\\n\")\n",
    "        f.write(str(best_result.config))\n",
    "\n",
    "    print(f\"\\nSaved pretty ASCII summary to: {summary_path}\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e2e9f",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGULAR_TRAINING:\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device, split=\"test\")\n",
    "\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd4f64",
   "metadata": {},
   "source": [
    "# Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d70b0",
   "metadata": {},
   "source": [
    "## Epoch plot: Train / Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGGING_ACTIVE:\n",
    "    if optimizer is not None:\n",
    "        epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        plt.plot(epochs_range, train_acc_history, marker=\"o\", label=\"Train accuracy\")\n",
    "        plt.plot(epochs_range, val_acc_history, marker=\"o\", label=\"Validation accuracy\")\n",
    "\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Accuracy over epochs\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # Create folder for plots if it doesn't exist\n",
    "        plots_dir = ROOT / \"plots\"\n",
    "        plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Use your logging filename for uniqueness\n",
    "        plot_path = plots_dir / f\"{run_name}_accuracy.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved accuracy plot to: {plot_path}\")\n",
    "\n",
    "        # (Optional) also log to W&B\n",
    "        wandb.log({\"plots/accuracy_curve\": wandb.Image(str(plot_path))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation + Top-N Confusion Logging ---\n",
    "if LOGGING_ACTIVE:\n",
    "    model.eval()   # IMPORTANT: inference mode\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, preds = outputs.max(1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f673b",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGGING_ACTIVE:\n",
    "    cm = confusion_matrix(all_labels, all_preds)    # Confusion matrix\n",
    "\n",
    "    # Top-N confused class pairs\n",
    "    N = 20\n",
    "\n",
    "    off = cm.copy()\n",
    "    np.fill_diagonal(off, 0)\n",
    "\n",
    "    indices = np.dstack(\n",
    "        np.unravel_index(np.argsort(off.ravel())[::-1], off.shape)\n",
    "    )[0][:N]\n",
    "\n",
    "    rows = []\n",
    "    for (t, p) in indices:\n",
    "        rows.append([class_names[t], class_names[p], int(cm[t, p])])\n",
    "\n",
    "    table = wandb.Table(\n",
    "        columns=[\"true_class\", \"predicted_class\", \"count\"],\n",
    "        data=rows\n",
    "    )\n",
    "\n",
    "    wandb.log({\"top_confusions\": table})\n",
    "\n",
    "    # Log Confusion Matrix Figure in W&B\n",
    "    fig = plt.figure(figsize=(30, 30), dpi=300)\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.title(\"Confusion Matrix (101×101)\", fontsize=28)\n",
    "    plt.xlabel(\"Predicted\", fontsize=24)\n",
    "    plt.ylabel(\"True\", fontsize=24)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.tight_layout(pad=0.3)\n",
    "\n",
    "    # Create temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
    "        tmp_path = tmp.name\n",
    "        plt.savefig(tmp_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({\"confusion_matrix_image\": wandb.Image(tmp_path)})\n",
    "\n",
    "    # Delete temporary file\n",
    "    os.remove(tmp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf575c55",
   "metadata": {},
   "source": [
    "## Per-class accuracy (sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-class accuracy sorted (best -> worst) ---\n",
    "if LOGGING_ACTIVE:\n",
    "    num_classes = len(class_names)\n",
    "    class_correct = np.zeros(num_classes, dtype=int)\n",
    "    class_total = np.zeros(num_classes, dtype=int)\n",
    "\n",
    "    for t, p in zip(all_labels, all_preds):\n",
    "        class_total[t] += 1\n",
    "        if t == p:\n",
    "            class_correct[t] += 1\n",
    "\n",
    "    class_accuracy = (class_correct / class_total)\n",
    "\n",
    "    # build sorted list\n",
    "    sorted_idx = np.argsort(class_accuracy)[::-1]  # best first\n",
    "\n",
    "    rows = []\n",
    "    for idx in sorted_idx:\n",
    "        rows.append([\n",
    "            class_names[idx],\n",
    "            float(class_accuracy[idx]),\n",
    "            int(class_correct[idx]),\n",
    "            int(class_total[idx])\n",
    "        ])\n",
    "\n",
    "    acc_table = wandb.Table(\n",
    "        columns=[\"class\", \"accuracy\", \"correct\", \"total\"],\n",
    "        data=rows\n",
    "    )\n",
    "\n",
    "    wandb.log({\"per_class_accuracy_sorted\": acc_table})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b627e",
   "metadata": {},
   "source": [
    "## Top 10 most misclassified classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGGING_ACTIVE:\n",
    "    # Find top misclassified classes\n",
    "    num_classes   = len(class_names)\n",
    "    class_correct = np.zeros(num_classes, dtype=int)\n",
    "    class_total   = np.zeros(num_classes, dtype=int)\n",
    "\n",
    "    for t, p in zip(all_labels, all_preds):\n",
    "        class_total[t] += 1\n",
    "        if t == p:\n",
    "            class_correct[t] += 1\n",
    "\n",
    "    class_errors = class_total - class_correct\n",
    "    sorted_err_idx = np.argsort(class_errors)[::-1] # Sort classes by number of misclassifications\n",
    "\n",
    "    top_k = 10\n",
    "    top_mis_classes = sorted_err_idx[:top_k]\n",
    "\n",
    "    # Collect misclassified images for these classes\n",
    "    images_by_class = {cls: [] for cls in top_mis_classes}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, preds = outputs.max(1)\n",
    "\n",
    "            mismatch = preds != labels\n",
    "\n",
    "            for img, pred, true in zip(imgs[mismatch], preds[mismatch], labels[mismatch]):\n",
    "                t = true.item()\n",
    "                if t in images_by_class and len(images_by_class[t]) < 10:\n",
    "                    images_by_class[t].append(\n",
    "                        wandb.Image(\n",
    "                            img.cpu(),\n",
    "                            caption=f\"true={class_names[t]}, pred={class_names[pred.item()]}\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            # stop early if all 10 classes reached 10 images\n",
    "            if all(len(v) >= 10 for v in images_by_class.values()):\n",
    "                break\n",
    "\n",
    "    # Log grouped images to W&B\n",
    "    log_dict = {}\n",
    "    for cls in top_mis_classes:\n",
    "        name = class_names[cls]\n",
    "        log_dict[f\"misclassified/{name}\"] = images_by_class[cls]\n",
    "\n",
    "    wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a2458",
   "metadata": {},
   "source": [
    "## GradCam for wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was generated by Chat-GPT\n",
    "\n",
    "if LOGGING_ACTIVE:\n",
    "    \n",
    "    # 1. Grad-CAM class\n",
    "    class GradCAM:\n",
    "        def __init__(self, model, target_layer):\n",
    "            self.model = model\n",
    "            self.target_layer = target_layer\n",
    "\n",
    "            self.gradients = None\n",
    "            self.activations = None\n",
    "\n",
    "            target_layer.register_forward_hook(self._save_activation)\n",
    "            target_layer.register_full_backward_hook(self._save_gradient)\n",
    "\n",
    "        def _save_activation(self, module, inp, out):\n",
    "            self.activations = out\n",
    "\n",
    "        def _save_gradient(self, module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        def __call__(self, x, class_idx):\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x)\n",
    "            loss = out[:, class_idx]\n",
    "            loss.backward()\n",
    "\n",
    "            weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "            cam = (weights * self.activations).sum(dim=1)\n",
    "\n",
    "            # Normalization with zero-protection\n",
    "            cam = F.relu(cam)\n",
    "            cam = cam - cam.min()\n",
    "\n",
    "            max_val = cam.max()\n",
    "            if max_val > 0:\n",
    "                cam = cam / max_val\n",
    "            else:\n",
    "                cam = torch.zeros_like(cam)\n",
    "\n",
    "            return cam\n",
    "\n",
    "    # 2. Correct EfficientNet-B0 target layer\n",
    "    target_layer = model.features[7][0].block[1][0]\n",
    "\n",
    "    # enable gradients only on this layer\n",
    "    for p in target_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "    # 3. Overlay helper\n",
    "    def overlay_cam(img_tensor, cam):\n",
    "        img = img_tensor.numpy().transpose(1, 2, 0)\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "        cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(cam * 255), cv2.COLORMAP_JET)\n",
    "        heatmap = heatmap.astype(np.float32) / 255.0\n",
    "\n",
    "        overlay = img * 0.5 + heatmap * 0.5\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        return overlay\n",
    "\n",
    "    # Heat Legend (colorbar)\n",
    "    def generate_colorbar_with_text(height=300, width=60):\n",
    "        \"\"\"\n",
    "        Returns a (H,W,3) numpy image containing:\n",
    "        - JET colormap\n",
    "        - Labels 'High', 'Mid', 'Low'\n",
    "        - Numeric ticks 1.0, 0.5, 0.0\n",
    "        \"\"\"\n",
    "        # Create vertical gradient\n",
    "        gradient = np.linspace(1, 0, height).reshape(height, 1)\n",
    "        gradient = np.repeat(gradient, width, axis=1)\n",
    "        gradient_uint8 = np.uint8(gradient * 255)\n",
    "        jet = cv2.applyColorMap(gradient_uint8, cv2.COLORMAP_JET)\n",
    "        jet = cv2.cvtColor(jet, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Create a canvas for text (slightly wider)\n",
    "        canvas = np.ones((height, width + 120, 3), dtype=np.float32)\n",
    "        canvas[:, :width] = jet / 255.0\n",
    "\n",
    "        # Add text using cv2\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        cv2.putText(canvas, \"High (1.0)\", (width + 10, 20), font, 0.5, (1, 1, 1), 1, cv2.LINE_AA)           # High (red)\n",
    "        cv2.putText(canvas, \"Mid (0.5)\", (width + 10, height // 2), font, 0.5, (1, 1, 1), 1, cv2.LINE_AA)   # Middle\n",
    "        cv2.putText(canvas, \"Low (0.0)\", (width + 10, height - 10), font, 0.5, (1, 1, 1), 1, cv2.LINE_AA)   # Low (blue)\n",
    "\n",
    "        return canvas\n",
    "\n",
    "    # Generate + log legend\n",
    "    colorbar_img = generate_colorbar_with_text()\n",
    "    wandb.log({\"gradcam/color_scale\": wandb.Image(colorbar_img)})\n",
    "\n",
    "    # 4. Grad-CAM on misclassified images\n",
    "    gradcam_results = []\n",
    "    model.eval()\n",
    "\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        _, preds = outputs.max(1)\n",
    "\n",
    "        mismatch = preds != labels\n",
    "        mismatch_idx = torch.where(mismatch)[0]\n",
    "\n",
    "        for idx in mismatch_idx:\n",
    "            img = imgs[idx].unsqueeze(0)\n",
    "            class_idx = preds[idx].item()\n",
    "\n",
    "            cam = gradcam(img, class_idx=class_idx)[0].detach().cpu().numpy()\n",
    "\n",
    "            overlay = overlay_cam(imgs[idx].cpu(), cam)\n",
    "\n",
    "            gradcam_results.append(\n",
    "                wandb.Image(\n",
    "                    overlay,\n",
    "                    caption=f\"true={class_names[labels[idx].item()]}, pred={class_names[preds[idx].item()]}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if len(gradcam_results) >= 20:\n",
    "                break\n",
    "\n",
    "        if len(gradcam_results) >= 20:\n",
    "            break\n",
    "\n",
    "    # 5. Log all Grad-CAM images\n",
    "    wandb.log({\"gradcam/misclassified\": gradcam_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede8bd1",
   "metadata": {},
   "source": [
    "## Misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGGING_ACTIVE:\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, preds = outputs.max(1)\n",
    "\n",
    "            mismatch = preds != labels\n",
    "            if mismatch.any():\n",
    "                for img, pred, true in zip(imgs[mismatch], preds[mismatch], labels[mismatch]):\n",
    "                    misclassified_images.append(img.cpu())\n",
    "                    misclassified_preds.append(pred.cpu().item())\n",
    "                    misclassified_labels.append(true.cpu().item())\n",
    "\n",
    "                    # keep it small\n",
    "                    if len(misclassified_images) >= 32:\n",
    "                        break\n",
    "            if len(misclassified_images) >= 32:\n",
    "                break\n",
    "\n",
    "    # Log to W&B as images\n",
    "    wandb.log({\n",
    "        \"test/misclassified_examples\": [\n",
    "            wandb.Image(\n",
    "                img,\n",
    "                caption=f\"pred: {class_names[p]}, true: {class_names[t]}\"\n",
    "            )\n",
    "            for img, p, t in zip(misclassified_images, misclassified_preds, misclassified_labels)\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGGING_ACTIVE:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
