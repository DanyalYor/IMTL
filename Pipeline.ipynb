{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1553cb2f",
   "metadata": {},
   "source": [
    "# Settings script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff9c3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL SELECTION ---\n",
    "MODEL_SCRATCH = False     # No pretrained weights, trained from scratch\n",
    "MODEL_BASELINE = False    # Pretrained EfficientNet, but NOT trained on Food-101\n",
    "MODEL_FINETUNE = True     # Pretrained EfficientNet, THEN fine-tuned on Food-101\n",
    "\n",
    "# --- OTHER SETTINGS ---\n",
    "RUN_RAYTUNE = False\n",
    "USE_FULL_DATASET = False    # If false, use small dataset instead (10K pictures, 100 per class)\n",
    "NUM_EPOCHS = 10\n",
    "MANUAL_DATA_AUGMENTATION = False\n",
    "LOGGING_FILENAME = \"efficientnet_b0_food101_run_9\"\n",
    "\n",
    "# --- Sanity check: EXACTLY one model active ---\n",
    "assert sum([MODEL_BASELINE, MODEL_SCRATCH, MODEL_FINETUNE]) == 1, \\\n",
    "    \"Exactly one model must be selected.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6492d",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd3a7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56810e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "# Standard library\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "# ML\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "# ML\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TorchVision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "\n",
    "# W&B\n",
    "import wandb\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ray Tune\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11ed7",
   "metadata": {},
   "source": [
    "# Set GPU variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c8e7652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f85172",
   "metadata": {},
   "source": [
    "# Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "223add30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IMAGE_DIR = C:\\Users\\EG\\OneDrive - ITU\\Kandidat\\3. semester\\AML - Computer Vision\\IMTL\\data\\food-101-small\n"
     ]
    }
   ],
   "source": [
    "ROOT = pathlib.Path().resolve()\n",
    "\n",
    "if USE_FULL_DATASET:\n",
    "    IMAGE_DIR = ROOT / \"data\" / \"food-101-big\"\n",
    "else:\n",
    "    IMAGE_DIR = ROOT / \"data\" / \"food-101-small\"\n",
    "\n",
    "print(\"Using IMAGE_DIR =\", IMAGE_DIR)\n",
    "\n",
    "if not IMAGE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing dataset folder:\\n{IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338636e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5b3e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 10100\n",
      "Classes: 101\n"
     ]
    }
   ],
   "source": [
    "img_size = 224 # Needed width and height dimensions for EfficientNet\n",
    "\n",
    "# Use AutoAugment for data augmentation\n",
    "if MANUAL_DATA_AUGMENTATION:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()\n",
    "    ])    \n",
    "\n",
    "else:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        AutoAugment(policy=AutoAugmentPolicy.IMAGENET),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(IMAGE_DIR, transform=train_transform, allow_empty=True)\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"Total images:\", len(full_dataset))\n",
    "print(\"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc7548",
   "metadata": {},
   "source": [
    "# Split data into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef3477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.70 * len(full_dataset))\n",
    "val_size   = int(0.15 * len(full_dataset))\n",
    "train_size = int(0.70 * len(full_dataset))\n",
    "val_size   = int(0.15 * len(full_dataset))\n",
    "test_size  = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a56d32",
   "metadata": {},
   "source": [
    "# Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b1a6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.dataset.transform = test_transform\n",
    "test_ds.dataset.transform = test_transform\n",
    "\n",
    "batch_size_default = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size_default, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size_default, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size_default, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db9857",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0e5ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model but NOT trained (baseline)\n",
    "def build_model_baseline(lr, num_classes):\n",
    "    from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "    import torch.nn as nn\n",
    "\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    # Freeze all parameters → no training\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    optimizer = None \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "# Train from scratch\n",
    "def build_model_scratch(lr, num_classes):\n",
    "    from torchvision.models import efficientnet_b0\n",
    "    import torch.nn as nn\n",
    "    import torch\n",
    "\n",
    "    model = efficientnet_b0(weights=None)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "# Pretrained → fine-tuned\n",
    "def build_model_finetune(lr, num_classes):\n",
    "    from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "    import torch.nn as nn\n",
    "    import torch\n",
    "\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "\n",
    "# def build_model_1(lr, num_classes):\n",
    "#     \"\"\"EfficientNet-B0 model\"\"\"\n",
    "#     weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "#     model = efficientnet_b0(weights=weights)\n",
    "\n",
    "#     model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "#     for p in model.features.parameters():\n",
    "#         p.requires_grad = False\n",
    "\n",
    "#     model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     return model, optimizer, criterion\n",
    "\n",
    "\n",
    "# def build_model_2(lr, num_classes):\n",
    "#     \"\"\"Placeholder model — intentionally blank\"\"\"\n",
    "#     raise NotImplementedError(\"MODEL_2 not implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9727ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_BASELINE:\n",
    "    build_model = build_model_baseline\n",
    "elif MODEL_SCRATCH:\n",
    "    build_model = build_model_scratch\n",
    "elif MODEL_FINETUNE:\n",
    "    build_model = build_model_finetune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b997e6e",
   "metadata": {},
   "source": [
    "# Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ff7f3",
   "metadata": {},
   "source": [
    "## Base training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "79d1f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch, log_interval=100):\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "\n",
    "    for batch_idx, (imgs, labels) in enumerate(pbar):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        avg_loss_so_far = total_loss / total\n",
    "        avg_acc_so_far = correct / total\n",
    "        avg_loss_so_far = total_loss / total\n",
    "        avg_acc_so_far = correct / total\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{avg_loss_so_far:.4f}\",\n",
    "            \"acc\": f\"{100 * avg_acc_so_far:.2f}%\"\n",
    "        })\n",
    "\n",
    "        # Optional: batch-level logging to W&B\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"batch/train_loss\": loss.item(),\n",
    "                    \"batch/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                },\n",
    "                step=epoch\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = correct / total\n",
    "\n",
    "    # Epoch-level logging to W&B\n",
    "    wandb.log({\n",
    "        \"train/loss\": avg_loss,\n",
    "        \"train/accuracy\": avg_acc,\n",
    "        \"train/error_rate\": 1 - avg_acc,\n",
    "    }, step=epoch)\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device, epoch=None, split=\"val\"):\n",
    "    \"\"\"\n",
    "    Generic evaluation for val/test.\n",
    "    split: \"val\" or \"test\" (used in W&B metric names).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    desc = f\"Evaluating ({split})\"\n",
    "    if epoch is not None:\n",
    "        desc += f\" Epoch {epoch}\"\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    desc = f\"Evaluating ({split})\"\n",
    "    if epoch is not None:\n",
    "        desc += f\" Epoch {epoch}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=desc, leave=False)\n",
    "\n",
    "        for imgs, labels in pbar:\n",
    "        pbar = tqdm(loader, desc=desc, leave=False)\n",
    "\n",
    "        for imgs, labels in pbar:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            avg_loss_so_far = total_loss / total\n",
    "            avg_acc_so_far = correct / total\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_loss_so_far:.4f}\",\n",
    "                \"acc\": f\"{100 * avg_acc_so_far:.2f}%\"\n",
    "            })\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = correct / total\n",
    "\n",
    "    # Log to W&B if epoch is known (for val) or just once (for test)\n",
    "    metric_prefix = f\"{split}\"\n",
    "    log_data = {\n",
    "        f\"{metric_prefix}/loss\": avg_loss,\n",
    "        f\"{metric_prefix}/accuracy\": avg_acc,\n",
    "        f\"{metric_prefix}/error_rate\": 1 - avg_acc,\n",
    "    }\n",
    "    if epoch is not None:\n",
    "        log_data[\"epoch\"] = epoch\n",
    "\n",
    "    wandb.log(log_data, step=epoch if epoch is not None else wandb.run.step)\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d527f",
   "metadata": {},
   "source": [
    "## RayTune training (Hyperparameter Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2ff717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_raytune(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def train_one_epoch_raytune(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def tune_train(config):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "\n",
    "    # local transforms\n",
    "    train_ds.dataset.transform = train_transform\n",
    "    val_ds.dataset.transform   = test_transform\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    model, optimizer, criterion = build_model(lr, num_classes)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_one_epoch_raytune(model, train_loader, optimizer, criterion, device)\n",
    "        train_one_epoch_raytune(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        tune.report({\"loss\": float(val_loss), \"accuracy\": float(val_acc)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0464e41",
   "metadata": {},
   "source": [
    "# Run RayTune (If toggled on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b16b994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_RAYTUNE:\n",
    "    os.environ[\"RAY_DISABLE_METRICS_EXPORT\"] = \"1\"\n",
    "\n",
    "    search_space = {\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "        \"batch_size\": tune.choice([16, 32, 64]),\n",
    "        \"epochs\": 3\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(metric=\"accuracy\", mode=\"max\")\n",
    "    reporter  = CLIReporter(metric_columns=[\"loss\", \"accuracy\"])\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune_train,\n",
    "            resources={\"cpu\": 4, \"gpu\": 1 if torch.cuda.is_available() else 0},\n",
    "        ),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            num_samples=6,\n",
    "        ),\n",
    "        run_config=tune.RunConfig(progress_reporter=reporter),\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n",
    "\n",
    "    print(\"Best config:\", best.config)\n",
    "\n",
    "    best_config = best.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c129c0c",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "777d5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_RAYTUNE:\n",
    "    model, optimizer, criterion = build_model(1e-3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f51894",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff97a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\marku\\OneDrive - ITU\\Kandidat\\3. semester\\AML - Computer Vision\\IMTL\\wandb\\run-20251127_140352-16h61dfn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/16h61dfn' target=\"_blank\">efficientnet_b0_food101_run_6</a></strong> to <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL' target=\"_blank\">https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/16h61dfn' target=\"_blank\">https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/16h61dfn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not RUN_RAYTUNE:\n",
    "    # W&B init\n",
    "    wandb.login()  # will prompt you the first time in this environment\n",
    "\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    def get_run_name(lr, batch_size):\n",
    "        lr_string = str(lr)\n",
    "        batch_size_string = str(batch_size)\n",
    "        date_string = str(datetime.datetime.now())\n",
    "        return f\"lr{lr_string}_bs{batch_size_string}_{date_string}\"\n",
    "\n",
    "    wandb_config = {\n",
    "        \"model_name\": \"efficientnet_b0\",\n",
    "        \"img_size\": img_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"train_size\": len(train_ds),\n",
    "        \"val_size\": len(val_ds),\n",
    "        \"test_size\": len(test_ds),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"transfer_learning\": True,      \n",
    "        \"feature_extractor_frozen\": True,\n",
    "        \"augmentation\": \"AutoAugment_IMAGENET\"\n",
    "    }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"IMTL\",\n",
    "        config=wandb_config,\n",
    "        name=get_run_name(learning_rate, batch_size),\n",
    "    )\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c129c0c",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "777d5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8417, Acc: 0.2143\n",
      "Val   Loss: 3.1640, Acc: 0.3426\n",
      "\n",
      "--- Epoch 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6542, Acc: 0.4614\n",
      "Val   Loss: 2.6948, Acc: 0.4026\n",
      "\n",
      "--- Epoch 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1690, Acc: 0.5366\n",
      "Val   Loss: 2.4803, Acc: 0.4271\n",
      "\n",
      "--- Epoch 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9051, Acc: 0.5777\n",
      "Val   Loss: 2.3912, Acc: 0.4422\n",
      "\n",
      "--- Epoch 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6965, Acc: 0.6232\n",
      "Val   Loss: 2.3381, Acc: 0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if not RUN_RAYTUNE:\n",
    "    model, optimizer, criterion = build_model(1e-3, num_classes)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f51894",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff97a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\EG\\OneDrive - ITU\\Kandidat\\3. semester\\AML - Computer Vision\\IMTL\\wandb\\run-20251202_123859-5bgcctg5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/5bgcctg5' target=\"_blank\">efficientnet_b0_food101_run_9</a></strong> to <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL' target=\"_blank\">https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/5bgcctg5' target=\"_blank\">https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/5bgcctg5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not RUN_RAYTUNE:\n",
    "    # W&B init\n",
    "    wandb.login()  # will prompt you the first time in this environment\n",
    "\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    def get_run_name(lr, batch_size):\n",
    "        lr_string = str(lr)\n",
    "        batch_size_string = str(batch_size)\n",
    "        date_string = str(datetime.datetime.now())\n",
    "        return f\"lr{lr_string}_bs{batch_size_string}_{date_string}\"\n",
    "\n",
    "    wandb_config = {\n",
    "        \"model_name\": \"efficientnet_b0\",\n",
    "        \"img_size\": img_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"train_size\": len(train_ds),\n",
    "        \"val_size\": len(val_ds),\n",
    "        \"test_size\": len(test_ds),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"transfer_learning\": True,      \n",
    "        \"feature_extractor_frozen\": True,\n",
    "        \"augmentation\": \"AutoAugment_IMAGENET\"\n",
    "    }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"IMTL\",\n",
    "        config=wandb_config,\n",
    "        name=get_run_name(learning_rate, batch_size),\n",
    "    )\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829f54c",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a73d035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0849, Acc: 0.2813\n",
      "Val   Loss: 2.3874, Acc: 0.3974\n",
      "\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7448, Acc: 0.5436\n",
      "Val   Loss: 1.9461, Acc: 0.5162\n",
      "\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0933, Acc: 0.6999\n",
      "Val   Loss: 1.9143, Acc: 0.5366\n",
      "\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6933, Acc: 0.8013\n",
      "Val   Loss: 1.9575, Acc: 0.5498\n",
      "\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5028, Acc: 0.8491\n",
      "Val   Loss: 2.0760, Acc: 0.5518\n",
      "\n",
      "--- Epoch 6/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3579, Acc: 0.8965\n",
      "Val   Loss: 2.1434, Acc: 0.5228\n",
      "\n",
      "--- Epoch 7/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3018, Acc: 0.9074\n",
      "Val   Loss: 2.2364, Acc: 0.5452\n",
      "\n",
      "--- Epoch 8/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2560, Acc: 0.9231\n",
      "Val   Loss: 2.3704, Acc: 0.5380\n",
      "\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2504, Acc: 0.9290\n",
      "Val   Loss: 2.5851, Acc: 0.5380\n",
      "\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2454, Acc: 0.9245\n",
      "Val   Loss: 2.5062, Acc: 0.5281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if optimizer is None:\n",
    "    print(\"\\n=== Baseline model selected — skipping training ===\")\n",
    "    # Run validation ONCE so you still see how bad the logits are\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "else:\n",
    "    epochs = NUM_EPOCHS\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        val_loss, val_acc     = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e2e9f",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc4b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Results ===\n",
      "Test Loss:     2.4314\n",
      "Test Accuracy: 0.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if not RUN_RAYTUNE:\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device, split=\"test\")\n",
    "\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd4f64",
   "metadata": {},
   "source": [
    "# Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "906d52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation + Top-N Confusion Logging ---\n",
    "\n",
    "model.eval()   # IMPORTANT: inference mode\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        _, preds = outputs.max(1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f673b",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c57f39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion matrix ---\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# --- Top-N confused class pairs (better than 101×101 heatmap) ---\n",
    "N = 20  # adjust as needed\n",
    "\n",
    "off = cm.copy()\n",
    "np.fill_diagonal(off, 0)\n",
    "\n",
    "indices = np.dstack(np.unravel_index(\n",
    "    np.argsort(off.ravel())[::-1], off.shape\n",
    "))[0][:N]\n",
    "\n",
    "rows = []\n",
    "for (t, p) in indices:\n",
    "    rows.append([\n",
    "        class_names[t],\n",
    "        class_names[p],\n",
    "        int(cm[t, p])\n",
    "    ])\n",
    "\n",
    "table = wandb.Table(\n",
    "    columns=[\"true_class\", \"predicted_class\", \"count\"],\n",
    "    data=rows\n",
    ")\n",
    "\n",
    "wandb.log({\"top_confusions\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf575c55",
   "metadata": {},
   "source": [
    "## Per-class accuracy (sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7cdb993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-class accuracy sorted (best -> worst) ---\n",
    "\n",
    "num_classes = len(class_names)\n",
    "class_correct = np.zeros(num_classes, dtype=int)\n",
    "class_total = np.zeros(num_classes, dtype=int)\n",
    "\n",
    "for t, p in zip(all_labels, all_preds):\n",
    "    class_total[t] += 1\n",
    "    if t == p:\n",
    "        class_correct[t] += 1\n",
    "\n",
    "class_accuracy = (class_correct / class_total)\n",
    "\n",
    "# build sorted list\n",
    "sorted_idx = np.argsort(class_accuracy)[::-1]  # best first\n",
    "\n",
    "rows = []\n",
    "for idx in sorted_idx:\n",
    "    rows.append([\n",
    "        class_names[idx],\n",
    "        float(class_accuracy[idx]),\n",
    "        int(class_correct[idx]),\n",
    "        int(class_total[idx])\n",
    "    ])\n",
    "\n",
    "acc_table = wandb.Table(\n",
    "    columns=[\"class\", \"accuracy\", \"correct\", \"total\"],\n",
    "    data=rows\n",
    ")\n",
    "\n",
    "wandb.log({\"per_class_accuracy_sorted\": acc_table})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b627e",
   "metadata": {},
   "source": [
    "## Top 10 most misclassified classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f06d1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find top misclassified classes ---\n",
    "\n",
    "num_classes = len(class_names)\n",
    "class_correct = np.zeros(num_classes, dtype=int)\n",
    "class_total   = np.zeros(num_classes, dtype=int)\n",
    "\n",
    "for t, p in zip(all_labels, all_preds):\n",
    "    class_total[t] += 1\n",
    "    if t == p:\n",
    "        class_correct[t] += 1\n",
    "\n",
    "class_errors = class_total - class_correct\n",
    "\n",
    "# sort classes by number of misclassifications\n",
    "sorted_err_idx = np.argsort(class_errors)[::-1]\n",
    "\n",
    "top_k = 10\n",
    "top_mis_classes = sorted_err_idx[:top_k]\n",
    "\n",
    "# --- Collect misclassified images for these classes ---\n",
    "\n",
    "images_by_class = {cls: [] for cls in top_mis_classes}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        _, preds = outputs.max(1)\n",
    "\n",
    "        mismatch = preds != labels\n",
    "\n",
    "        for img, pred, true in zip(imgs[mismatch], preds[mismatch], labels[mismatch]):\n",
    "            t = true.item()\n",
    "            if t in images_by_class and len(images_by_class[t]) < 10:\n",
    "                images_by_class[t].append(\n",
    "                    wandb.Image(\n",
    "                        img.cpu(),\n",
    "                        caption=f\"true={class_names[t]}, pred={class_names[pred.item()]}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # stop early if all 10 classes reached 10 images\n",
    "        if all(len(v) >= 10 for v in images_by_class.values()):\n",
    "            break\n",
    "\n",
    "# --- Log grouped images to W&B ---\n",
    "\n",
    "log_dict = {}\n",
    "for cls in top_mis_classes:\n",
    "    name = class_names[cls]\n",
    "    log_dict[f\"misclassified/{name}\"] = images_by_class[cls]\n",
    "\n",
    "wandb.log(log_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a2458",
   "metadata": {},
   "source": [
    "## GradCam for wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78f8cd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\Anaconda\\envs\\ml310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "C:\\Users\\EG\\AppData\\Local\\Temp\\ipykernel_25364\\4097003045.py:58: RuntimeWarning: invalid value encountered in cast\n",
      "  heatmap = cv2.applyColorMap(np.uint8(cam * 255), cv2.COLORMAP_JET)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Grad-CAM class\n",
    "# ----------------------------\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        target_layer.register_forward_hook(self._save_activation)\n",
    "        target_layer.register_backward_hook(self._save_gradient)\n",
    "\n",
    "    def _save_activation(self, module, inp, out):\n",
    "        self.activations = out\n",
    "\n",
    "    def _save_gradient(self, module, grad_in, grad_out):\n",
    "        self.gradients = grad_out[0]\n",
    "\n",
    "    def __call__(self, x, class_idx):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = out[:, class_idx]\n",
    "        loss.backward()\n",
    "\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1)\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max()\n",
    "\n",
    "        return cam\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Correct EfficientNet-B0 target layer\n",
    "# ----------------------------\n",
    "target_layer = model.features[7][0].block[1][0]\n",
    "\n",
    "# enable gradients only on this layer\n",
    "for p in target_layer.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Overlay helper\n",
    "# ----------------------------\n",
    "def overlay_cam(img_tensor, cam):\n",
    "    img = img_tensor.numpy().transpose(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(cam * 255), cv2.COLORMAP_JET)\n",
    "    heatmap = heatmap.astype(np.float32) / 255.0\n",
    "\n",
    "    overlay = img * 0.5 + heatmap * 0.5\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3.5  SINGLE HEAT LEGEND (colorbar)\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# 3.5  SINGLE HEAT LEGEND (colorbar with text)\n",
    "# ----------------------------\n",
    "def generate_colorbar_with_text(height=300, width=60):\n",
    "    \"\"\"\n",
    "    Returns a (H,W,3) numpy image containing:\n",
    "    - JET colormap\n",
    "    - Labels 'High', 'Mid', 'Low'\n",
    "    - Numeric ticks 1.0, 0.5, 0.0\n",
    "    \"\"\"\n",
    "    # Create vertical gradient\n",
    "    gradient = np.linspace(1, 0, height).reshape(height, 1)\n",
    "    gradient = np.repeat(gradient, width, axis=1)\n",
    "    gradient_uint8 = np.uint8(gradient * 255)\n",
    "    jet = cv2.applyColorMap(gradient_uint8, cv2.COLORMAP_JET)\n",
    "    jet = cv2.cvtColor(jet, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a canvas for text (slightly wider)\n",
    "    canvas = np.ones((height, width + 120, 3), dtype=np.float32)\n",
    "    canvas[:, :width] = jet / 255.0\n",
    "\n",
    "    # Add text using cv2\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    cv2.putText(canvas, \"High (1.0)\", (width + 10, 20), font, 0.5, (1, 1, 1), 1, cv2.LINE_AA)           # High (red)\n",
    "    cv2.putText(canvas, \"Mid (0.5)\", (width + 10, height // 2), font, 0.5, (1, 1, 1), 1, cv2.LINE_AA)   # Middle\n",
    "    cv2.putText(canvas, \"Low (0.0)\", (width + 10, height - 10), font, 0.5, (1, 1, 1), 1, cv2.LINE_AA)   # Low (blue)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "# Generate + log legend\n",
    "colorbar_img = generate_colorbar_with_text()\n",
    "wandb.log({\"gradcam/color_scale\": wandb.Image(colorbar_img)})\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Grad-CAM on misclassified images\n",
    "# ----------------------------\n",
    "gradcam_results = []\n",
    "model.eval()\n",
    "\n",
    "for imgs, labels in test_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model(imgs)\n",
    "    _, preds = outputs.max(1)\n",
    "\n",
    "    mismatch = preds != labels\n",
    "    mismatch_idx = torch.where(mismatch)[0]\n",
    "\n",
    "    for idx in mismatch_idx:\n",
    "        img = imgs[idx].unsqueeze(0)\n",
    "        class_idx = preds[idx].item()\n",
    "\n",
    "        cam = gradcam(img, class_idx=class_idx)[0].detach().cpu().numpy()\n",
    "\n",
    "        overlay = overlay_cam(imgs[idx].cpu(), cam)\n",
    "\n",
    "        gradcam_results.append(\n",
    "            wandb.Image(\n",
    "                overlay,\n",
    "                caption=f\"true={class_names[labels[idx].item()]}, pred={class_names[preds[idx].item()]}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if len(gradcam_results) >= 20:\n",
    "            break\n",
    "\n",
    "    if len(gradcam_results) >= 20:\n",
    "        break\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Log all Grad-CAM images\n",
    "# ----------------------------\n",
    "wandb.log({\"gradcam/misclassified\": gradcam_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede8bd1",
   "metadata": {},
   "source": [
    "## Misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "55ba19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_images = []\n",
    "misclassified_preds = []\n",
    "misclassified_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        _, preds = outputs.max(1)\n",
    "\n",
    "        mismatch = preds != labels\n",
    "        if mismatch.any():\n",
    "            for img, pred, true in zip(imgs[mismatch], preds[mismatch], labels[mismatch]):\n",
    "                misclassified_images.append(img.cpu())\n",
    "                misclassified_preds.append(pred.cpu().item())\n",
    "                misclassified_labels.append(true.cpu().item())\n",
    "\n",
    "                # keep it small\n",
    "                if len(misclassified_images) >= 32:\n",
    "                    break\n",
    "        if len(misclassified_images) >= 32:\n",
    "            break\n",
    "\n",
    "# Log to W&B as images\n",
    "wandb.log({\n",
    "    \"test/misclassified_examples\": [\n",
    "        wandb.Image(\n",
    "            img,\n",
    "            caption=f\"pred: {class_names[p]}, true: {class_names[t]}\"\n",
    "        )\n",
    "        for img, p, t in zip(misclassified_images, misclassified_preds, misclassified_labels)\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd28443d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/lr</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/train_loss</td><td>█▆▅▄▃▂▁▁▂▁</td></tr><tr><td>train/accuracy</td><td>▁▄▆▇▇█████</td></tr><tr><td>train/error_rate</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▆▇██▇█▇▇▇</td></tr><tr><td>val/error_rate</td><td>█▃▂▁▁▂▁▂▂▂</td></tr><tr><td>val/loss</td><td>▆▁▁▁▃▃▄▆█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/lr</td><td>0.001</td></tr><tr><td>batch/train_loss</td><td>0.27734</td></tr><tr><td>train/accuracy</td><td>0.92447</td></tr><tr><td>train/error_rate</td><td>0.07553</td></tr><tr><td>train/loss</td><td>0.2454</td></tr><tr><td>val/accuracy</td><td>0.53465</td></tr><tr><td>val/error_rate</td><td>0.46535</td></tr><tr><td>val/loss</td><td>2.4314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficientnet_b0_food101_run_9</strong> at: <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/5bgcctg5' target=\"_blank\">https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL/runs/5bgcctg5</a><br> View project at: <a href='https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL' target=\"_blank\">https://wandb.ai/dard-it-universitetet-i-k-benhavn/IMTL</a><br>Synced 5 W&B file(s), 155 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251202_123859-5bgcctg5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
